{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Local RAG Pipeline with Local Embeddings and Generation\n",
    "\n",
    "\n",
    "\n",
    " This script sets up a Retrieval-Augmented Generation (RAG) pipeline that runs entirely on your local machine.\n",
    "\n",
    " - **Embedding Model**: `Qwen/Qwen3-Embedding-0.6B` running via `sentence-transformers`.\n",
    "\n",
    " - **Vector Database**: ChromaDB for storing and retrieving document embeddings.\n",
    "\n",
    " - **Generation Model**: Llama 3.2 (or any other model) running via Ollama."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 1. Setup and Installations\n",
    "\n",
    " Install the necessary libraries for the pipeline. `sentence-transformers` and `torch` are added to run the local embedding model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For running generation via local model\n",
    "%pip install ollama\n",
    "# For running local embedding model\n",
    "%pip install sentence-transformers torch\n",
    "# Core libraries for RAG\n",
    "%pip install chromadb pandas python-dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import chromadb\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "from dotenv import load_dotenv\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import ollama\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 2. Load and Prepare Documents\n",
    "\n",
    " These functions parse a JSONL file containing codebase information, format it for embedding, and handle chunking for large functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_metadata(meta_dict):\n",
    "    \"\"\"\n",
    "    Converts any dict or list values in a metadata dictionary to JSON strings,\n",
    "    as required by ChromaDB.\n",
    "    \"\"\"\n",
    "    flat_meta = {}\n",
    "    for key, value in meta_dict.items():\n",
    "        if isinstance(value, (dict, list)):\n",
    "            try:\n",
    "                # Attempt to serialize to JSON string\n",
    "                flat_meta[key] = json.dumps(value)\n",
    "            except TypeError:\n",
    "                # Fallback for non-serializable objects\n",
    "                flat_meta[key] = str(value)\n",
    "        elif value is None:\n",
    "            continue # Skip None values\n",
    "        else:\n",
    "            flat_meta[key] = value\n",
    "    return flat_meta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_prepare_docs(filepath=\"codebase_map.jsonl\", max_lines=50, overlap_lines=5):\n",
    "    \"\"\"\n",
    "    Loads the JSONL file and formats each entry for embedding.\n",
    "    Handles all code types: structs, functions, imports, constants, variables, and interfaces.\n",
    "    If a function's body exceeds max_lines, it's split into a parent document\n",
    "    and multiple child documents (body chunks).\n",
    "    \"\"\"\n",
    "    documents = []\n",
    "    metadata = []\n",
    "    ids = []\n",
    "    doc_counter = 1\n",
    "\n",
    "    # Check if the file exists before trying to open it\n",
    "    if not os.path.exists(filepath):\n",
    "        print(f\"Error: The file '{filepath}' was not found.\")\n",
    "        print(\"Please ensure the codebase map file is in the correct directory.\")\n",
    "        return [], [], []\n",
    "\n",
    "    with open(filepath, 'r') as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            content = \"\"\n",
    "            doc_type = data['type']\n",
    "\n",
    "            # Use a single metadata object for each simple entry\n",
    "            current_meta = flatten_metadata(data)\n",
    "\n",
    "            # Handle Structs\n",
    "            if doc_type == 'struct':\n",
    "                struct = data['struct']\n",
    "                fields_str_parts = []\n",
    "                for field in struct.get('fields', []):\n",
    "                    tag_str = f\"`{field.get('tag', '')}`\" if field.get('tag') else \"\"\n",
    "                    fields_str_parts.append(f\"  {field.get('name')} {field.get('type')} {tag_str}\".strip())\n",
    "                fields_str = \"\\n\".join(fields_str_parts)\n",
    "                content = f\"File: {data['file_path']}\\nType: struct\\nName: {struct['name']}\\nFields:\\n{fields_str}\"\n",
    "\n",
    "            # Handle Imports\n",
    "            elif doc_type == 'import':\n",
    "                imp = data['import']\n",
    "                alias_str = f\" as {imp['name']}\" if imp.get('name') else \"\"\n",
    "                content = f\"File: {data['file_path']}\\nType: import\\nStatement: import {imp['path']}{alias_str}\"\n",
    "\n",
    "            # Handle Constants and Variables\n",
    "            elif doc_type in ['constant', 'variable']:\n",
    "                spec = data[doc_type]\n",
    "                names_str = \", \".join(spec.get('names', []))\n",
    "                type_str = f\"\\nType: {spec['type']}\" if spec.get('type') else \"\"\n",
    "                value_str = f\"\\nValue: {spec['value']}\" if spec.get('value') else \"\"\n",
    "                content = f\"File: {data['file_path']}\\nDeclaration: {doc_type}\\nName(s): {names_str}{type_str}{value_str}\"\n",
    "\n",
    "            # Handle Interfaces\n",
    "            elif doc_type == 'interface':\n",
    "                interface = data['interface']\n",
    "                methods_str = \"\\n\".join([f\"  {method['signature']}\" for method in interface.get('methods', [])])\n",
    "                content = f\"File: {data['file_path']}\\nType: interface\\nName: {interface['name']}\\nMethods:\\n{methods_str}\"\n",
    "\n",
    "            # Handle Functions\n",
    "            elif doc_type == 'function':\n",
    "                func = data['function']\n",
    "                body_lines = func.get('body', '').split('\\n')\n",
    "\n",
    "                if len(body_lines) > max_lines:\n",
    "                    # Parent document for a large function\n",
    "                    parent_content = f\"File: {data['file_path']}\\nType: function\\nSignature: {func['signature']}\\nSummary: This is a large function with its body broken into smaller chunks.\"\n",
    "                    parent_id = str(doc_counter)\n",
    "                    documents.append(parent_content)\n",
    "\n",
    "                    parent_meta = copy.deepcopy(data)\n",
    "                    parent_meta['function']['body'] = \"# BODY CHUNKED, SEE CHILD DOCUMENTS #\"\n",
    "                    metadata.append(flatten_metadata(parent_meta))\n",
    "                    ids.append(parent_id)\n",
    "                    doc_counter += 1\n",
    "\n",
    "                    # Child documents for each chunk\n",
    "                    step_size = max_lines - overlap_lines\n",
    "                    chunk_num = 1\n",
    "                    for i in range(0, len(body_lines), step_size):\n",
    "                        chunk_text = \"\\n\".join(body_lines[i:i + max_lines])\n",
    "                        if not chunk_text.strip(): continue\n",
    "\n",
    "                        chunk_content = (\n",
    "                            f\"File: {data['file_path']}\\n\"\n",
    "                            f\"Type: function_chunk\\n\"\n",
    "                            f\"Parent Function: {func['signature']}\\n\"\n",
    "                            f\"Chunk {chunk_num}:\\n---\\n{chunk_text}\"\n",
    "                        )\n",
    "                        documents.append(chunk_content)\n",
    "                        child_meta = {\n",
    "                            \"file_path\": data['file_path'],\n",
    "                            \"parent_function_name\": func['name'],\n",
    "                            \"is_chunk\": True,\n",
    "                            \"chunk_number\": chunk_num,\n",
    "                            \"parent_id\": parent_id\n",
    "                        }\n",
    "                        metadata.append(child_meta)\n",
    "                        ids.append(f\"{parent_id}_{chunk_num}\")\n",
    "                        doc_counter += 1\n",
    "                        chunk_num += 1\n",
    "                else:\n",
    "                    # Process normal-sized functions\n",
    "                    content = f\"File: {data['file_path']}\\nType: function\\nSignature: {func['signature']}\\nBody: {func['body']}\"\n",
    "\n",
    "            # For all non-chunked types, add the document\n",
    "            if content:\n",
    "                documents.append(content)\n",
    "                metadata.append(current_meta)\n",
    "                ids.append(str(doc_counter))\n",
    "                doc_counter += 1\n",
    "\n",
    "    return documents, metadata, ids\n",
    "\n",
    "# --- Main execution ---\n",
    "print(\"Loading and preparing documents...\")\n",
    "# NOTE: Make sure you have a 'codebase_map.jsonl' file in the same directory.\n",
    "documents, metadata, ids = load_and_prepare_docs(filepath=\"codebase_map.jsonl\")\n",
    "\n",
    "if documents:\n",
    "    print(f\"Loaded and processed {len(documents)} documents.\")\n",
    "    # Example: Print a few documents to see the formats\n",
    "    for i, doc in enumerate(documents[:3]):\n",
    "        print(f\"\\n--- Document {i+1} ---\\n{doc}\")\n",
    "else:\n",
    "    print(\"No documents were loaded. Please check the file path and content.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 3. Initialize Local Embedding Model\n",
    "\n",
    " We load the `Qwen/Qwen3-Embedding-0.6B` model from Hugging Face. This model will run on your local machine to generate embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Initializing local embedding model... This might take a moment.\")\n",
    "# Using trust_remote_code=True is required for some models on Hugging Face.\n",
    "embedding_model = SentenceTransformer(\n",
    "    'Qwen/Qwen3-Embedding-0.6B',\n",
    "    trust_remote_code=True\n",
    ")\n",
    "print(\"Embedding model loaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 4. Embed and Store in VectorDB\n",
    "\n",
    " This cell takes the prepared documents, generates embeddings for them using the local Qwen model, and stores them in a local ChromaDB collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if documents:\n",
    "    # Initialize ChromaDB client.\n",
    "    client = chromadb.PersistentClient(path=\"./chroma_db_local\")\n",
    "    collection_name = \"gocodebase_local_qwen\"\n",
    "\n",
    "    # Check if the collection already exists to avoid re-indexing\n",
    "    if collection_name in [c.name for c in client.list_collections()]:\n",
    "        print(f\"Collection '{collection_name}' already exists with {client.get_collection(name=collection_name).count()} items. Skipping indexing.\")\n",
    "        collection = client.get_collection(name=collection_name)\n",
    "    else:\n",
    "        print(f\"Collection '{collection_name}' not found. Creating and indexing now...\")\n",
    "        collection = client.create_collection(name=collection_name)\n",
    "        print(f\"Created a new collection: '{collection_name}'\")\n",
    "\n",
    "        print(\"Embedding and indexing the codebase with local model... This may take a while.\")\n",
    "        batch_size = 20 # Using a smaller batch size for local processing\n",
    "        for i in range(0, len(documents), batch_size):\n",
    "            batch_docs = documents[i:i+batch_size]\n",
    "            batch_ids = ids[i:i+batch_size]\n",
    "            batch_meta = metadata[i:i+batch_size]\n",
    "\n",
    "            # Generate embeddings locally\n",
    "            embeddings = embedding_model.encode(\n",
    "                batch_docs,\n",
    "                normalize_embeddings=True # Normalizing is good practice for retrieval\n",
    "            ).tolist() # Convert to list for ChromaDB\n",
    "\n",
    "            collection.add(\n",
    "                embeddings=embeddings,\n",
    "                documents=batch_docs,\n",
    "                metadatas=batch_meta,\n",
    "                ids=batch_ids\n",
    "            )\n",
    "            print(f\"Indexed batch {i//batch_size + 1} of {len(documents)//batch_size + 1}...\")\n",
    "\n",
    "        print(\"Codebase successfully indexed in ChromaDB.\")\n",
    "        item_count = collection.count()\n",
    "        print(f\"The collection now has {item_count} items.\")\n",
    "else:\n",
    "    print(\"Skipping embedding process as no documents were loaded.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 5. Query the RAG Pipeline\n",
    "\n",
    " The `query_rag` function orchestrates the process:\n",
    "\n",
    " 1. Takes a user query.\n",
    "\n",
    " 2. Generates an embedding for the query using the local Qwen model.\n",
    "\n",
    " 3. Retrieves relevant documents from ChromaDB.\n",
    "\n",
    " 4. Constructs a detailed prompt with the retrieved context.\n",
    "\n",
    " 5. Sends the prompt to a local LLM (e.g., Llama 3.2) via Ollama to generate the final response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_rag(query: str, collection, n_results: int = 5):\n",
    "    \"\"\"Performs the RAG process using local models.\"\"\"\n",
    "\n",
    "    # 1. Retrieve relevant code snippets using the local embedding model\n",
    "    query_embedding = embedding_model.encode(\n",
    "        query,\n",
    "        normalize_embeddings=True\n",
    "    ).tolist()\n",
    "\n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_embedding],\n",
    "        n_results=n_results\n",
    "    )\n",
    "\n",
    "\n",
    "    retrieved_docs = results['documents'][0]\n",
    "    context = \"\\n---\\n\".join(retrieved_docs)\n",
    "\n",
    "    # 2. Augment: Create a prompt for the local generative model\n",
    "    prompt = f\"\"\"You are an expert Go programmer. Your task is to help a user modify their codebase.\n",
    "Use the following relevant code snippets from the codebase as context to provide a complete and accurate answer.\n",
    "Some snippets might be chunks of larger functions, indicated by 'Type: function_chunk'. Use the parent function signature to understand the context.\n",
    "\n",
    "**CONTEXT FROM THE CODEBASE:**\n",
    "---\n",
    "{context}\n",
    "---\n",
    "\n",
    "**USER'S REQUEST:**\n",
    "\"{query}\"\n",
    "\n",
    "**YOUR TASK:**\n",
    "Based on the user's request and the provided context, generate the necessary code changes.\n",
    "- If a struct needs modification, show the new struct definition.\n",
    "- If a const needs modification, show the new const.\n",
    "- If a function needs to be changed, provide the complete, updated function body.\n",
    "- If new functions are needed, write them.\n",
    "- Provide a brief, clear explanation of the changes you made.\n",
    "- Present the final output in Go code blocks.\n",
    "\"\"\"\n",
    "    # print(\"--- PROMPT SENT TO LLAMA ---\")\n",
    "    # print(prompt)\n",
    "    # print(\"--------------------------\")\n",
    "\n",
    "    # 3. Generate the response from the local model via Ollama\n",
    "    print(\"Sending request to local Llama3.2 model...\")\n",
    "    try:\n",
    "        response = ollama.chat(\n",
    "            model='llama3.2', # Make sure this model is available in Ollama\n",
    "            messages=[\n",
    "                {'role': 'user', 'content': prompt},\n",
    "            ]\n",
    "        )\n",
    "        return response['message']['content']\n",
    "    except Exception as e:\n",
    "        return f\"Error communicating with Ollama: {e}\\n\\nPlease ensure Ollama is running and the 'llama3.2' model is installed. You can run 'ollama run llama3.2' in your terminal.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 6. Give a Request and Get a Response\n",
    "\n",
    " Now, we provide a request to modify the codebase and display the generated response from the local RAG pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "user_request = \"\"\"\n",
    "I need to add a 'likes' count to the Chirp model.\n",
    "It should be an integer and default to 0.\n",
    "\n",
    "Then, update the 'HandlerChirpsCreate' function. After creating a chirp,\n",
    "the response should include this new 'likes' field.\n",
    "\"\"\"\n",
    "\n",
    "# Connect to the persistent ChromaDB and get the collection to query\n",
    "print(\"Connecting to local ChromaDB to run query...\")\n",
    "client = chromadb.PersistentClient(path=\"./chroma_db_local\")\n",
    "collection_name = \"gocodebase_local_qwen\"\n",
    "\n",
    "try:\n",
    "    collection_to_query = client.get_collection(name=collection_name)\n",
    "    print(f\"Successfully connected to collection '{collection_name}'.\")\n",
    "\n",
    "    # Get the suggested code change by passing the collection to the function\n",
    "    suggested_change = query_rag(user_request, collection=collection_to_query)\n",
    "\n",
    "    # Create the full markdown string and display it\n",
    "    markdown_output = f\"\"\"\n",
    "---\n",
    "### SUGGESTED CODE CHANGE\n",
    "---\n",
    "{suggested_change}\n",
    "\"\"\"\n",
    "\n",
    "    display(Markdown(markdown_output))\n",
    "except ValueError:\n",
    "    print(f\"\\nError: Collection '{collection_name}' not found in the database.\")\n",
    "    print(\"Please make sure you have run the embedding and indexing cell (Cell #4) at least once.\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
